{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import thunderkittens as tk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"You are using `torch.load` with `weights_only=False`\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = \"/home/bfs/simran/attention/nanoGPT-TK/\"\n",
    "files = os.listdir(fpath)\n",
    "files = [f for f in files if f.endswith(\".pt\")]\n",
    "\n",
    "files1 = [f for f in files if \"1\" in f]\n",
    "files2 = [f for f in files if \"2\" in f]\n",
    "files3 = [f for f in files if \"3\" in f]\n",
    "files4 = [f for f in files if \"4\" in f]\n",
    "files5 = [f for f in files if \"5\" in f]\n",
    "files6 = [f for f in files if \"6\" in f]\n",
    "files7 = [f for f in files if \"7\" in f]\n",
    "files8 = [f for f in files if \"8\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1q.pt', '1k.pt', '1v.pt', '1o.pt']\n",
      "tensor(False, device='cuda:0') tensor(False, device='cuda:0') tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "['2q.pt', '2k.pt', '2v.pt', '2o.pt']\n",
      "tensor(False, device='cuda:0') tensor(False, device='cuda:0') tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "['3q.pt', '3k.pt', '3v.pt', '3o.pt']\n",
      "tensor(False, device='cuda:0') tensor(False, device='cuda:0') tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "['4q.pt', '4k.pt', '4v.pt', '4o.pt']\n",
      "tensor(False, device='cuda:0') tensor(False, device='cuda:0') tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "['5q.pt', '5k.pt', '5v.pt', '5o.pt']\n",
      "tensor(False, device='cuda:0') tensor(False, device='cuda:0') tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "['6q.pt', '6k.pt', '6v.pt', '6o.pt']\n",
      "tensor(False, device='cuda:0') tensor(False, device='cuda:0') tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "['7q.pt', '7k.pt', '7v.pt', '7o.pt']\n",
      "tensor(False, device='cuda:0') tensor(False, device='cuda:0') tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n",
      "['8q.pt', '8k.pt', '8v.pt', '8o.pt']\n",
      "tensor(False, device='cuda:0') tensor(False, device='cuda:0') tensor(False, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def run_attn(q, k, v, o, l_vec):\n",
    "    tk.mha_forward(q, k, v, o, l_vec, True) \n",
    "    torch.cuda.synchronize()\n",
    "    return o\n",
    "\n",
    "for fs in [files1, files2, files3, files4, files5, files6, files7, files8]:\n",
    "    print(fs)\n",
    "    q_path = [f for f in fs if \"q\" in f][0]\n",
    "    k_path = [f for f in fs if \"k\" in f][0]\n",
    "    v_path = [f for f in fs if \"v\" in f][0]\n",
    "    o_path = [f for f in fs if \"o\" in f][0]\n",
    "    q = torch.load(fpath + q_path, weights_only=True)\n",
    "    k = torch.load(fpath + k_path, weights_only=True)\n",
    "    v = torch.load(fpath + v_path, weights_only=True)\n",
    "    o = torch.load(fpath + o_path, weights_only=True)\n",
    "\n",
    "    # are there nans in the q, k, v tensors?\n",
    "    print(torch.isnan(q).any(), torch.isnan(k).any(), torch.isnan(v).any())\n",
    "\n",
    "    # sanity check with randn instead of above q, k, v\n",
    "    # q = torch.randn_like(q, dtype=torch.bfloat16, device='cuda')\n",
    "    # k = torch.randn_like(k, dtype=torch.bfloat16, device='cuda')\n",
    "    # v = torch.randn_like(v, dtype=torch.bfloat16, device='cuda')\n",
    "\n",
    "    b, h, n, d = q.shape\n",
    "    # print(q.shape, k.shape, v.shape, o.shape)\n",
    "    outputs = torch.empty((b, h, n, d), dtype=torch.bfloat16, device='cuda')\n",
    "    l_vec   = torch.empty((b, h, n, 1), dtype=torch.float32, device='cuda')\n",
    "\n",
    "    run_attn(q, k, v, outputs, l_vec)\n",
    "\n",
    "    ################ CHECKS ################\n",
    "    # are there nans?\n",
    "    print(torch.isnan(outputs).any())\n",
    "\n",
    "    # compare outputs (computed here) and o (saved during training)?\n",
    "    # print(outputs.shape, o.shape)\n",
    "    max_diff = torch.max(torch.abs(outputs - o))\n",
    "\n",
    "    # where are the non-zeros in the tensor?\n",
    "    # print(torch.isnan(outputs).nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isnan(outputs).any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
