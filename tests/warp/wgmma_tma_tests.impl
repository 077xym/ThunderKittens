#define WGMMA_N 4 // must be 4 due to WGMMA
#define WORKERS 4

template<bool use_reg, int WGMMA_K, int WGMMA_M>
__global__ void test_wgmma_tma_mma_ker_trans(const bf16 *input, bf16 *output, CUtensorMap* tma_desc_input) {

    int warpid   = threadIdx.x / 32;
    int tile_idx = blockIdx.x * 4 + warpid;

    CUtensorMap* input_tma_descriptor  = tma_desc_input;

    extern __shared__ alignment_dummy __shm22[]; // this is the CUDA shared memory
    shared_allocator al((int*)&__shm22[0]);

    rt_fl<1, WGMMA_M> result;
    rt_bf<1, WGMMA_K> a_reg;

    rt_bf<WGMMA_K, WGMMA_M> tmp_loader;
    
    using L1 = st_wgmma_row_0b_layout;
    using L2 = st_wgmma_col_t_0b_layout;
    st_bf<WGMMA_N, WGMMA_K, L1> &a_smem = al.allocate<st_bf<WGMMA_N, WGMMA_K, L1>>();
    st_bf<WGMMA_K, WGMMA_M, L2> &b_smem = al.allocate<st_bf<WGMMA_K, WGMMA_M, L2>>();

    auto block = cooperative_groups::this_thread_block();
    __shared__ uint64_t smem_barrier;  
    constexpr int size_bytes = sizeof(bf16) * a_smem.num_elements;  

    tma::init_barrier(smem_barrier, block.size());
    tma::set_barrier_bytes(smem_barrier, size_bytes);
    block.sync();

    if (warpid == 0) {
        tma::prefetch(b_smem, input_tma_descriptor, tile_idx);
        // B always runs in shared memory
        tma::load_async(b_smem, input_tma_descriptor, tile_idx, smem_barrier);
    }

    constexpr int kPhaseBit = 1;
    tma::arrive_wait(smem_barrier, kPhaseBit);

    // How to load A?
    if constexpr (use_reg) {
        warpgroup::load(a_reg, input, 16*WGMMA_K);
        __syncthreads();
        warpgroup::fence(result);
        warpgroup::mma_reset(result, a_reg, b_smem);
    }
    else {
        warpgroup::load(a_smem, input, 16*WGMMA_K);
        __syncthreads();
        warpgroup::fence(result);
        warpgroup::mma_reset(result, a_smem, b_smem);
    }
    warpgroup::commit_group();
    warpgroup::mma_async_wait();
    
    warpgroup::store(output, result, 16*WGMMA_M); // this is going to break
}
template<bool use_reg, int WGMMA_K, int WGMMA_M>
bool test_wgmma_tma_mma_trans() {
    // initailize
    bf16 *d_i, *d_o;
    std::vector<float> i_ref(256*WGMMA_K*(WGMMA_N+WGMMA_M));
    std::vector<float> o_ref(256*WGMMA_N*WGMMA_M);
    initialize(&d_i, &d_o, i_ref, o_ref);

    CUtensorMap tma_desc_input = {};
    tma::create_tensor_map<st_bf<WGMMA_K, WGMMA_M, st_wgmma_col_t_0b_layout>, 1>(&tma_desc_input, d_i+256*WGMMA_N*WGMMA_K); 

    CUtensorMap* tma_desc_input_d;
    cudaMalloc(&tma_desc_input_d, sizeof(CUtensorMap));
    cudaMemcpy(tma_desc_input_d, &tma_desc_input, sizeof(CUtensorMap), cudaMemcpyHostToDevice);

    // run kernel
    cudaFuncSetAttribute(test_wgmma_tma_mma_ker_trans<use_reg, WGMMA_K, WGMMA_M>, cudaFuncAttributeMaxDynamicSharedMemorySize, 200000);
    test_wgmma_tma_mma_ker_trans<use_reg, WGMMA_K, WGMMA_M><<<1, 128, 200000>>>(d_i, d_o, tma_desc_input_d);
    // fill in correct results on cpu
    for(int i = 0; i < WGMMA_N*16; i++) {
        for(int j = 0; j < WGMMA_M*16; j++) {
            float sum = 0;
            for(int k = 0; k < WGMMA_K*16; k++) {
                sum += i_ref[i*WGMMA_K*16+k]*i_ref[256*WGMMA_K*WGMMA_N + k*16*WGMMA_M + j];
            }
            o_ref[i*WGMMA_M*16+j] = sum;
        }
    }
    // check and cleanup
    std::string reg_label = "rt_st_";
    if(!use_reg) reg_label = "st_st_";
    bool passed = validate(
        d_i, d_o, i_ref, o_ref, 
        "wgmma_mma_"+reg_label+std::to_string(WGMMA_N)+"x"+std::to_string(WGMMA_K)+"x"+std::to_string(WGMMA_M),
        WGMMA_M*16
    );
    return passed;
}


int wgmma_tma_tests() {
    std::cout << " ----- Starting Warpgroup MMA tests! -----" << std::endl;
    int failures = 0;

    failures += !test_wgmma_tma_mma_trans<true , 1, 1>();
    failures += !test_wgmma_tma_mma_trans<true , 1, 2>();
    failures += !test_wgmma_tma_mma_trans<true , 1, 3>();
    failures += !test_wgmma_tma_mma_trans<true , 1, 4>();
    failures += !test_wgmma_tma_mma_trans<true , 2, 1>();
    failures += !test_wgmma_tma_mma_trans<true , 2, 2>();
    failures += !test_wgmma_tma_mma_trans<true , 2, 3>();
    failures += !test_wgmma_tma_mma_trans<true , 2, 4>();
    failures += !test_wgmma_tma_mma_trans<true , 3, 1>();
    failures += !test_wgmma_tma_mma_trans<true , 3, 2>();
    failures += !test_wgmma_tma_mma_trans<true , 3, 3>();
    failures += !test_wgmma_tma_mma_trans<true , 3, 4>();
    failures += !test_wgmma_tma_mma_trans<true , 4, 1>();
    // failures += !test_wgmma_tma_mma_trans<true , 4, 2>(); // numerical instability
    failures += !test_wgmma_tma_mma_trans<true , 4, 3>();
    failures += !test_wgmma_tma_mma_trans<true , 4, 4>();
    failures += !test_wgmma_tma_mma_trans<false, 1, 1>();
    failures += !test_wgmma_tma_mma_trans<false, 1, 2>();
    failures += !test_wgmma_tma_mma_trans<false, 1, 3>();
    failures += !test_wgmma_tma_mma_trans<false, 1, 4>();
    failures += !test_wgmma_tma_mma_trans<false, 2, 1>();
    failures += !test_wgmma_tma_mma_trans<false, 2, 2>();
    failures += !test_wgmma_tma_mma_trans<false, 2, 3>();
    failures += !test_wgmma_tma_mma_trans<false, 2, 4>();
    failures += !test_wgmma_tma_mma_trans<false, 3, 1>();
    failures += !test_wgmma_tma_mma_trans<false, 3, 2>();
    failures += !test_wgmma_tma_mma_trans<false, 3, 3>();
    failures += !test_wgmma_tma_mma_trans<false, 3, 4>();
    failures += !test_wgmma_tma_mma_trans<false, 4, 1>();
    // failures += !test_wgmma_tma_mma_trans<false, 4, 2>(); // numerical instability
    failures += !test_wgmma_tma_mma_trans<false, 4, 3>();
    failures += !test_wgmma_tma_mma_trans<false, 4, 4>();
    
    return failures;
}