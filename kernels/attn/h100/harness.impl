#include <iostream>
#include <string>
#include <fstream>

constexpr int ATTN_B     = 16;

constexpr int QO_HEADS   = 16;
constexpr int KV_HEADS   = 16;

constexpr int QK_HEAD_RATIO = (QO_HEADS)/(KV_HEADS);
static_assert(QO_HEADS >= KV_HEADS && QO_HEADS % KV_HEADS == 0);

constexpr int ATTN_N     = 12288/4;
constexpr int ATTN_D     = 64; // hardcoded into this harness
constexpr int BLOCK_SIZE = (32*NUM_WORKERS);
constexpr int ITER       = 100;

constexpr bool causal    = false; 

#define CudaCheckError()    __cudaCheckError( __FILE__, __LINE__ )
inline void __cudaCheckError( const char *file, const int line ) {
    cudaError err = cudaGetLastError();
    if ( cudaSuccess != err )
    {
        fprintf( stderr, "cudaCheckError() failed at %s:%i : %s\n",
                 file, line, cudaGetErrorString( err ) );
        exit( -1 );
    }
    // More careful checking. However, this will affect performance.
    // Comment away if needed.
    err = cudaDeviceSynchronize();
    if( cudaSuccess != err )
    {
        fprintf( stderr, "cudaCheckError() with sync failed at %s:%i : %s\n",
                 file, line, cudaGetErrorString( err ) );
        exit( -1 );
    }
}

// Function to calculate the number of floating-point operations
long long flops(int batch, int seqlen, int headdim, int nheads, bool causal, const std::string& mode) {
    assert(mode == "fwd" || mode == "bwd" || mode == "fwd_bwd");
    long long f = 4 * batch * static_cast<long long>(seqlen) * seqlen * nheads * headdim;
    f /= (causal ? 2 : 1);

    if (mode == "fwd") {
        return f;
    } else if (mode == "bwd") {
        return static_cast<long long>(2.5 * f);
    } else { // fwd_bwd
        return static_cast<long long>(3.5 * f);
    }
}

// Function to calculate the efficiency in teraflops
double efficiency(long long flop, double time) {
    // Convert flop to teraflops and time to milliseconds
    double tflops = flop / 1e12;
    double time_ms = time / 1e6;
    return tflops / time_ms;
}

int main(int argc, char **argv) {
    // TODO: consider doing sequential kernel launches to force batches dimension element to execute sequentially,
    // which may increase the probability of L2 cache hits on KV

    std::cout << "Entered main!" << std::endl;

    // create dummy variables that are the right size
    constexpr int TOTAL_ELEMENTS_QO     = ATTN_B*QO_HEADS*ATTN_N*ATTN_D;
    constexpr int TOTAL_ELEMENTS_KV     = ATTN_B*KV_HEADS*ATTN_N*ATTN_D;
    constexpr int TOTAL_UNIQUE_ELEMENTS = ATTN_N*ATTN_D;

    float *q      = new float[TOTAL_ELEMENTS_QO/ATTN_B];
    float *k      = new float[TOTAL_ELEMENTS_KV/ATTN_B];
    float *v      = new float[TOTAL_ELEMENTS_KV/ATTN_B];
    float *o_ref  = new float[TOTAL_ELEMENTS_QO/ATTN_B];
    float *l_ref  = new float[TOTAL_ELEMENTS_QO/(ATTN_B * ATTN_D)];

    float *og     = new float[TOTAL_ELEMENTS_QO/ATTN_B];
    float *d_ref  = new float[TOTAL_ELEMENTS_QO/(ATTN_B * ATTN_D)];

    bf16  *q_bf = new bf16[TOTAL_ELEMENTS_QO];
    bf16  *k_bf = new bf16[TOTAL_ELEMENTS_KV];
    bf16  *v_bf = new bf16[TOTAL_ELEMENTS_KV];
    bf16  *o_bf = new bf16[TOTAL_ELEMENTS_QO];
    float *l_fl = new float[TOTAL_ELEMENTS_QO/ATTN_D];

    bf16 *og_bf  = new bf16[TOTAL_ELEMENTS_QO];
    float *d_fl  = new float[TOTAL_ELEMENTS_QO/ATTN_D]; 

    float *o = new float[TOTAL_ELEMENTS_QO];
    float *l = new float[TOTAL_ELEMENTS_QO/ATTN_D];

    float *d = new float[TOTAL_ELEMENTS_QO/ATTN_D];

    std::ifstream infile(argv[1]);

    std::cout << "Starting to enter!" << std::endl;

    for(int i = 0; i < TOTAL_ELEMENTS_QO/ATTN_B; i++) infile >> q[i];
    std::cout << "Finished loading Q" << std::endl;
    for(int i = 0; i < TOTAL_ELEMENTS_KV/ATTN_B; i++) infile >> k[i];
    std::cout << "Finished loading K" << std::endl;
    for(int i = 0; i < TOTAL_ELEMENTS_KV/ATTN_B; i++) infile >> v[i];
    std::cout << "Finished loading V" << std::endl;
    for(int i = 0; i < TOTAL_ELEMENTS_QO/ATTN_B; i++) infile >> o_ref[i];
    std::cout << "Finished loading O_REF" << std::endl;
    for(int i = 0; i < TOTAL_ELEMENTS_QO/(ATTN_B * ATTN_D); i++) infile >> l_ref[i];
    std::cout << "Finished loading L_REF" << std::endl;
    for (int i = 0; i < TOTAL_ELEMENTS_QO/(ATTN_B * ATTN_D); i++) infile >> d_ref[i];
    std::cout << "Finished loading D_REF" << std::endl; 
    for(int i = 0; i < TOTAL_ELEMENTS_QO/ATTN_B; i++) infile >> og[i];
    std::cout << "Finished loading OG" << std::endl;

    std::cout << "Finished loading file from " << argv[1] << "!" << std::endl;

    // replicate into heads
    // replicate into batch
    for(int i = 0; i < TOTAL_ELEMENTS_QO; i++) {
        q_bf[i]  = __float2bfloat16(q[i % (TOTAL_ELEMENTS_QO/ATTN_B)]);
        og_bf[i] = __float2bfloat16(og[i % (TOTAL_ELEMENTS_QO/ATTN_B)]);
    }
    for(int i = 0; i < TOTAL_ELEMENTS_KV; i++) {
        k_bf[i] = __float2bfloat16(k[i % (TOTAL_ELEMENTS_KV/ATTN_B)]);
        v_bf[i] = __float2bfloat16(v[i % (TOTAL_ELEMENTS_KV/ATTN_B)]);
    }

    bf16 *d_q, *d_k, *d_v, *d_o, *d_og; 
    float *d_l, *d_d;
    cudaMalloc(&d_q, (TOTAL_ELEMENTS_QO) * sizeof(bf16));
    cudaMalloc(&d_k, (TOTAL_ELEMENTS_KV) * sizeof(bf16));
    cudaMalloc(&d_v, (TOTAL_ELEMENTS_KV) * sizeof(bf16));
    cudaMalloc(&d_o, (TOTAL_ELEMENTS_QO) * sizeof(bf16));
    
    cudaMalloc(&d_l, (TOTAL_ELEMENTS_QO/ATTN_D) * sizeof(float));
    cudaMalloc(&d_d, (TOTAL_ELEMENTS_QO/ATTN_D) * sizeof(float)); 

    cudaMalloc(&d_og, (TOTAL_ELEMENTS_QO) * sizeof(bf16));

    cudaMemcpy(d_q, q_bf, TOTAL_ELEMENTS_QO * sizeof(bf16), cudaMemcpyHostToDevice);
    cudaMemcpy(d_k, k_bf, TOTAL_ELEMENTS_KV * sizeof(bf16), cudaMemcpyHostToDevice);
    cudaMemcpy(d_v, v_bf, TOTAL_ELEMENTS_KV * sizeof(bf16), cudaMemcpyHostToDevice);

    cudaMemcpy(d_og, og_bf, TOTAL_ELEMENTS_QO * sizeof(bf16), cudaMemcpyHostToDevice);

    CUtensorMap* tma_q_d = tma::allocate_and_create_tensor_map<        st_bf<fwd_attend_ker_tile_dims<ATTN_D>::qo_height, fwd_attend_ker_tile_dims<ATTN_D>::tile_width> >(d_q, ATTN_B*QO_HEADS*ATTN_N/(fwd_attend_ker_tile_dims<ATTN_D>::qo_height * 16));
    CUtensorMap* tma_k_d = tma::allocate_and_create_tensor_map<        st_bf<fwd_attend_ker_tile_dims<ATTN_D>::kv_height, fwd_attend_ker_tile_dims<ATTN_D>::tile_width> >(d_k, ATTN_B*KV_HEADS*ATTN_N/(fwd_attend_ker_tile_dims<ATTN_D>::kv_height * 16));
    CUtensorMap* tma_v_d = tma::allocate_and_create_tensor_map<        st_bf<fwd_attend_ker_tile_dims<ATTN_D>::kv_height, fwd_attend_ker_tile_dims<ATTN_D>::tile_width> >(d_v, ATTN_B*KV_HEADS*ATTN_N/(fwd_attend_ker_tile_dims<ATTN_D>::kv_height * 16));
    CUtensorMap* tma_o_d = tma::allocate_and_create_tensor_map<        st_bf<fwd_attend_ker_tile_dims<ATTN_D>::qo_height, fwd_attend_ker_tile_dims<ATTN_D>::tile_width> >(d_o, ATTN_B*QO_HEADS*ATTN_N/(fwd_attend_ker_tile_dims<ATTN_D>::qo_height * 16));
    CUtensorMap* tma_l_d = tma::allocate_and_create_tensor_map<col_vec<st_fl<fwd_attend_ker_tile_dims<ATTN_D>::qo_height, fwd_attend_ker_tile_dims<ATTN_D>::tile_width>>>(d_l, ATTN_B*QO_HEADS*ATTN_N/(fwd_attend_ker_tile_dims<ATTN_D>::qo_height * 16));

    std::cout << "Allocated and set memory on GPU!" << std::endl;
    
    unsigned long mem_size = kittens::MAX_SHARED_MEMORY; // need to launch two blocks if possible.
    
    cudaFuncSetAttribute(
        fwd_attend_ker<ATTN_D, causal>,
        cudaFuncAttributeMaxDynamicSharedMemorySize,
        mem_size
    );
    std::cout << "Set max dynamic memory!" << std::endl;

    dim3 grid(ATTN_N/(CONSUMER_WARPGROUPS*kittens::TILE_DIM*4), ATTN_B*QO_HEADS, 1);
    static_assert(ATTN_N % (CONSUMER_WARPGROUPS*kittens::TILE_DIM*4) == 0);
    cudaDeviceSynchronize();
    std::cout << "Starting warmup" << std::endl;
    for(int i = 0; i < ITER; i++) {    
        fwd_attend_ker<ATTN_D, causal><<<grid, BLOCK_SIZE, mem_size>>>(ATTN_N, QK_HEAD_RATIO, tma_q_d, tma_k_d, tma_v_d, tma_o_d, tma_l_d);
    }
    cudaDeviceSynchronize();
    std::cout << "Starting kernel" << std::endl;
    const auto start = std::chrono::high_resolution_clock::now();
    for(int i = 0; i < ITER; i++) {
        fwd_attend_ker<ATTN_D, causal><<<grid, BLOCK_SIZE, mem_size>>>(ATTN_N, QK_HEAD_RATIO, tma_q_d, tma_k_d, tma_v_d, tma_o_d, tma_l_d);
    }
    cudaDeviceSynchronize();
    const auto finish = std::chrono::high_resolution_clock::now();
    CudaCheckError();
    std::cout << "Finished kernel\n";
    
    // check correctness
    cudaMemcpy(o_bf, d_o, TOTAL_ELEMENTS_QO * sizeof(bf16), cudaMemcpyDeviceToHost);
    for(int i = 0; i < TOTAL_ELEMENTS_QO; i++) {
        o[i] = __bfloat162float(o_bf[i]);
    }
    cudaMemcpy(l_fl, d_l, TOTAL_ELEMENTS_QO/ATTN_D * sizeof(float), cudaMemcpyDeviceToHost);
    for (int i = 0; i < TOTAL_ELEMENTS_QO/ATTN_D; i++) {
        l[i] = l_fl[i]; 
    }

    bool good = true;
    std::ofstream o_ref_file("printouts/o_ref.txt");
    std::ofstream o_file("printouts/o.txt");
    std::ofstream diff_file("printouts/o_diff.txt");

    float total_diff = 0;
    for(int i = 0; i < TOTAL_ELEMENTS_QO; i++) {
        float diff = o[i] - o_ref[i % (TOTAL_ELEMENTS_QO/ATTN_B)];
        if (i < TOTAL_UNIQUE_ELEMENTS) {
            o_ref_file << o_ref[i] << ' ';
            o_file << o[i] << ' ';
            diff_file << diff << ' ';
        }
        if(abs(diff) > 0.01 || isnan(diff)) {
            good = false;
        }
        total_diff += abs(diff);
    }

    // print average difference
    std::cout << "Average o difference: " << total_diff / TOTAL_ELEMENTS_QO << std::endl;
    if (abs(total_diff / TOTAL_ELEMENTS_QO) < 1e-3) {
        good = true;
    }

    float total_diff_l = 0.0f;
    std::ofstream l_ref_file("printouts/l_ref.txt");
    std::ofstream l_file("printouts/l.txt");
    std::ofstream diff_l_file("printouts/diff_l.txt");
    for(int i = 0; i < (ATTN_B*QO_HEADS*ATTN_N); i++) {
        float diff = l[i] - l_ref[i % (QO_HEADS*ATTN_N)];
        if (i < ATTN_N) {
            l_ref_file << l_ref[i] << ' ';
            l_file << l[i] << ' ';
            diff_l_file << diff << ' ';
        }
        if(abs(diff) > 1 || isnan(diff)) {
            good = false;
        }
        total_diff_l += abs(diff);
    }

    // print average l diff
    std::cout << "Average l diff: " << total_diff_l / (ATTN_B*QO_HEADS*ATTN_N) << std::endl;

    if (abs(total_diff_l / (ATTN_B*QO_HEADS*ATTN_N)) < 1e-3) {
        good = true;
    }

    std::cout << "Average fwd execution time: " << std::chrono::duration_cast<std::chrono::microseconds>(finish - start).count() / ITER << " us" << std::endl;
    if(good) std::cout << "FWD Correct :)\n";
    else std::cout << "FWD Incorrect :(\n";

    // calculate efficiency
    long long f = flops(ATTN_B, ATTN_N, ATTN_D, QO_HEADS, causal, "fwd");
    double e = efficiency(f, std::chrono::duration_cast<std::chrono::microseconds>(finish - start).count() / ITER);
    std::cout << "Efficiency: " << e << " TFLOPS\n\n\n" << std::endl;

    // backward pass
    CUtensorMap* tma_b_o_d  = tma::allocate_and_create_tensor_map<        st_bf<4, ATTN_D/kittens::TILE_DIM> >(d_o,  (ATTN_B*QO_HEADS*ATTN_N)/(4 * 16));
    CUtensorMap* tma_b_og_d = tma::allocate_and_create_tensor_map<        st_bf<4, ATTN_D/kittens::TILE_DIM> >(d_og, (ATTN_B*QO_HEADS*ATTN_N)/(4 * 16));
    CUtensorMap* tma_b_d_d  = tma::allocate_and_create_tensor_map<col_vec<st_fl<4, ATTN_D/kittens::TILE_DIM>>>(d_d,  (ATTN_B*QO_HEADS*ATTN_N)/(4 * 16));

    cudaFuncSetAttribute(
        bwd_attend_prep_ker<ATTN_D>,
        cudaFuncAttributeMaxDynamicSharedMemorySize,
        mem_size
    );
    std::cout << "Set max dynamic memory!" << std::endl;

    dim3 grid_bwd(ATTN_N/(4*kittens::TILE_DIM*4), ATTN_B*QO_HEADS, 1);
    static_assert(ATTN_N % (4*kittens::TILE_DIM*4) == 0);
    cudaDeviceSynchronize();
    std::cout << "Starting bwd prep warmup" << std::endl;
    for(int i = 0; i < ITER; i++) {
        bwd_attend_prep_ker<ATTN_D><<<grid_bwd, (32*4), mem_size>>>(tma_b_o_d, tma_b_d_d, tma_b_og_d);
    }
    cudaDeviceSynchronize();
    std::cout << "Starting bwd prep kernel" << std::endl;
    const auto start_bwd = std::chrono::high_resolution_clock::now();
    for(int i = 0; i < ITER; i++) {
        bwd_attend_prep_ker<ATTN_D><<<grid_bwd, (32*4), mem_size>>>(tma_b_o_d, tma_b_d_d, tma_b_og_d);
    }
    cudaDeviceSynchronize();
    const auto finish_bwd = std::chrono::high_resolution_clock::now();
    CudaCheckError();
    std::cout << "Finished bwd prep kernel\n";

    cudaMemcpy(d_fl, d_d, TOTAL_ELEMENTS_QO/ATTN_D * sizeof(float), cudaMemcpyDeviceToHost);

    for(int i = 0; i < TOTAL_ELEMENTS_QO/ATTN_D; i++) {
        d[i]  = d_fl[i];
    }

    good = true;

    float total_diff_d = 0.0f;
    std::ofstream d_ref_file("printouts/d_ref.txt");
    std::ofstream d_file("printouts/d.txt");
    std::ofstream diff_d_file("printouts/diff_d.txt");
    for(int i = 0; i < ATTN_N * ATTN_B * QO_HEADS; i++) {
        float diff = d[i] - d_ref[i % (QO_HEADS*ATTN_N)];
        if (i < ATTN_N) {
            d_ref_file << d_ref[i] << ' ';
            d_file << d[i] << ' ';
            diff_d_file << diff << ' ';
        }
        if(abs(diff) > 0.015 || isnan(diff)) {
            good = false;
        }
        total_diff_d += abs(diff);
    }
    std::cout << "Average d diff: " << total_diff_d / (ATTN_N * ATTN_B * QO_HEADS) << std::endl;
    if (abs(total_diff_d / (ATTN_N * ATTN_B * QO_HEADS)) < 1e-2) {
        good = true;
    }

    std::cout << "Average bwd prep execution time: " << std::chrono::duration_cast<std::chrono::microseconds>(finish_bwd - start_bwd).count() / ITER << " us" << std::endl;
    if(good) std::cout << "BWD Prep Correct :)\n\n\n\n";
    else std::cout << "BWD Prep Incorrect :(\n\n\n\n";

    cudaFree(d_q);
    cudaFree(d_k);
    cudaFree(d_v);
    cudaFree(d_o);

    delete[] q, k, v, o, o_ref;
    delete[] q_bf, k_bf, v_bf, o_bf;

    return 0;
}