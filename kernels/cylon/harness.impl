#include <iostream>
#include <fstream>
#include <sstream>
#include <vector>
#include <cmath>
#include <cuda_runtime.h>
#include <cuda_bf16.h>

constexpr int LOAD_B            = 1;
constexpr int B                 = 528;
constexpr int H                 = 1;
constexpr int N                 = 4096;
constexpr int D_QK              = 64;
constexpr int D_VO              = 128;
constexpr int COLLABORATIVE_SMS = 1;

// Error checking macro
#define CudaCheckError() { \
    cudaError_t e=cudaGetLastError(); \
    if(e!=cudaSuccess) { \
        printf("Cuda failure %s:%d: '%s'\n",__FILE__,__LINE__,cudaGetErrorString(e)); \
        exit(0); \
    } \
}

uint64_t flops(int b, int n, int d_qk, int d_vo, int collab_size) {
    uint64_t flops_per_sm = (64*64*64*2 * 8 * 2); // 8 MMA's, two consumer warpgroups
    flops_per_sm *= n / 64;
    return flops_per_sm * b * collab_size;
}

std::vector<float> loadDataFromFile(const std::string& filename, int expectedSize) {
    std::vector<float> data;
    std::ifstream file(filename);
    
    if (!file.is_open()) {
        std::cerr << "Error opening file: " << filename << std::endl;
        return data;
    }

    float value;
    while (file >> value) {
        data.push_back(value);
    }

    if (data.size() != expectedSize) {
        std::cerr << "Warning: File " << filename << " contains " << data.size() 
                  << " elements, expected " << expectedSize << std::endl;
    }

    return data;
}

void convertToBF16(const std::vector<float>& input, std::vector<bf16>& output) {
    for (int i = 0; i < output.size(); ++i) {
        output[i] = __float2bfloat16(input[i%input.size()]);
    }
}

template<typename T>
void compareOutputs(const std::vector<T>& computed, const std::vector<float>& reference, int size, float tolerance) {
    int mismatches[4] = {0, 0, 0, 0};
    float total_error = 0.0f;
    float total_magnitude = 0.0f;

    for (int i = 0; i < size; ++i) {
        float comp;
        if constexpr (std::is_same_v<T, bf16>) {
            comp = __bfloat162float(computed[i]);
        } else if constexpr (std::is_same_v<T, float>) {
            comp = computed[i];
        }
        float ref = reference[i%reference.size()];
        float diff = std::abs(comp - ref);
        
        total_error += diff;
        total_magnitude += std::abs(ref);

        if (diff > 0.0001) mismatches[0]++;
        if (diff > 0.001) mismatches[1]++;
        if (diff > 0.01) mismatches[2]++;
        if (diff > 0.1) mismatches[3]++;

        if (diff > tolerance && mismatches[0] <= 10) {
            std::cout << "Mismatch at index " << i << ": Computed = " << comp << ", Reference = " << ref << std::endl;
        }
    }
    
    float avg_error = total_error / size;
    float avg_magnitude = total_magnitude / size;
    float avg_percent_diff = (avg_error / avg_magnitude) * 100;

    std::cout << "Mismatches > 0.0001: " << mismatches[0] << " (" << (float)mismatches[0] / size * 100 << "%)" << std::endl;
    std::cout << "Mismatches > 0.001: " << mismatches[1] << " (" << (float)mismatches[1] / size * 100 << "%)" << std::endl;
    std::cout << "Mismatches > 0.01: " << mismatches[2] << " (" << (float)mismatches[2] / size * 100 << "%)" << std::endl;
    std::cout << "Mismatches > 0.1: " << mismatches[3] << " (" << (float)mismatches[3] / size * 100 << "%)" << std::endl;
    std::cout << "Average error: " << avg_error << std::endl;
    std::cout << "Average magnitude of reference: " << avg_magnitude << std::endl;
    std::cout << "Average percent difference: " << avg_percent_diff << "%" << std::endl;
}

int main() {
    // Load data from files
    auto q_data = loadDataFromFile("q.txt", LOAD_B * H * N * D_QK);
    auto k_data = loadDataFromFile("k.txt", LOAD_B * H * N * D_QK);
    auto v_data = loadDataFromFile("v.txt", LOAD_B * H * N * D_VO);
    auto q_map_data = loadDataFromFile("q_map.txt", H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM);
    auto k_map_data = loadDataFromFile("k_map.txt", H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM);
    auto ref_output_data = loadDataFromFile("reference_output.txt", LOAD_B * H * N * D_VO);
    auto ref_kv_state_data = loadDataFromFile("reference_kv_state.txt", LOAD_B * H * STATE_PER_SM * COLLABORATIVE_SMS * D_QK * D_VO);
    std::cout << "Loaded data from files" << std::endl;

    // Create arrays and convert data to bf16
    std::vector<bf16> q(B * H * N * D_QK);
    std::vector<bf16> k(B * H * N * D_QK);
    std::vector<bf16> v(B * H * N * D_VO);
    std::vector<bf16> o(B * H * N * D_VO);
    std::vector<float> kv_state(B * H * STATE_PER_SM * COLLABORATIVE_SMS * D_QK * D_VO);
    std::vector<bf16> q_map(H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM);
    std::vector<bf16> k_map(H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM);

    convertToBF16(q_data, q);
    convertToBF16(k_data, k);
    convertToBF16(v_data, v);
    convertToBF16(q_map_data, q_map);
    convertToBF16(k_map_data, k_map);
    std::cout << "Converted data to bf16" << std::endl;

    // Create CUDA tensors
    bf16 *d_q, *d_k, *d_v, *d_o, *d_q_map, *d_k_map;
    float *d_kv_state;
    cudaMalloc(&d_q, B * H * N * D_QK * sizeof(bf16));
    cudaMalloc(&d_k, B * H * N * D_QK * sizeof(bf16));
    cudaMalloc(&d_v, B * H * N * D_VO * sizeof(bf16));
    cudaMalloc(&d_o, B * H * N * D_VO * sizeof(bf16));
    cudaMalloc(&d_kv_state, B * H * STATE_PER_SM * COLLABORATIVE_SMS * D_QK * D_VO * sizeof(float));
    cudaMalloc(&d_q_map, H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM * sizeof(bf16));
    cudaMalloc(&d_k_map, H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM * sizeof(bf16));
    std::cout << "Allocated CUDA tensors" << std::endl;

    // Copy data to GPU
    cudaMemcpy(d_q, q.data(), B * H * N * D_QK * sizeof(bf16), cudaMemcpyHostToDevice);
    cudaMemcpy(d_k, k.data(), B * H * N * D_QK * sizeof(bf16), cudaMemcpyHostToDevice);
    cudaMemcpy(d_v, v.data(), B * H * N * D_VO * sizeof(bf16), cudaMemcpyHostToDevice);
    cudaMemcpy(d_q_map, q_map.data(), H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM * sizeof(bf16), cudaMemcpyHostToDevice);
    cudaMemcpy(d_k_map, k_map.data(), H * COLLABORATIVE_SMS * D_QK * D_QK * STATE_PER_SM * sizeof(bf16), cudaMemcpyHostToDevice);
    std::cout << "Copied data to GPU" << std::endl;

    // Create tma descriptors
    CUtensorMap* q_tma        = tma::allocate_and_create_tensor_map<st_bf_4x4>(d_q, B * H * N / 64);
    CUtensorMap* k_tma        = tma::allocate_and_create_tensor_map<st_bf_4x4>(d_k, B * H * N / 64);
    CUtensorMap* v_tma        = tma::allocate_and_create_tensor_map<st_bf_4x4>(d_v, B * H * N / 64, 2);
    CUtensorMap* o_tma        = tma::allocate_and_create_tensor_map<st_bf_4x4>(d_o, B * H * N / 64, 2);
    CUtensorMap* kv_state_tma = tma::allocate_and_create_tensor_map<st_fl_4x4>(d_kv_state, B * H * COLLABORATIVE_SMS * STATE_PER_SM, 2);
    CUtensorMap* q_map_tma    = tma::allocate_and_create_tensor_map<st_bf_4x4>(d_q_map, H * COLLABORATIVE_SMS * STATE_PER_SM);
    CUtensorMap* k_map_tma    = tma::allocate_and_create_tensor_map<st_bf_4x4>(d_k_map, H * COLLABORATIVE_SMS * STATE_PER_SM);
    std::cout << "Created TMA descriptors" << std::endl;

    // Launch kernel
    cudaFuncSetAttribute(cylon_forwards, cudaFuncAttributeMaxDynamicSharedMemorySize, 225000);

    dim3 grid(B, H, COLLABORATIVE_SMS);
    std::cout << "Launching kernel with grid of " << B << "x" << H << "x" << COLLABORATIVE_SMS << " and blocks of " << NUM_THREADS << " threads" << std::endl;
    auto start = std::chrono::high_resolution_clock::now();

    cylon_forwards<<<grid, NUM_THREADS, 225000>>>(N, q_tma, k_tma, v_tma, o_tma, kv_state_tma, q_map_tma, k_map_tma);
    CudaCheckError();

    // Wait for kernel to finish
    cudaDeviceSynchronize();
    CudaCheckError();
    std::cout << "Kernel finished" << std::endl;

    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);
    std::cout << "Kernel execution time: " << duration.count() << " microseconds" << std::endl;
    uint64_t flops_executed = flops(B, N, D_QK, D_VO, COLLABORATIVE_SMS);
    std::cout << "FLOPs executed: " << flops_executed/1e12 << "T\n";
    std::cout << "TFLOPs achieved: " << flops_executed / duration.count() / 1e6 << std::endl;

    // Copy data from GPU
    cudaMemcpy(o.data(), d_o, B * H * N * D_VO * sizeof(bf16), cudaMemcpyDeviceToHost);
    cudaMemcpy(kv_state.data(), d_kv_state, B * H * STATE_PER_SM * COLLABORATIVE_SMS * D_QK * D_VO * sizeof(float), cudaMemcpyDeviceToHost);

    // Compare outputs
    std::cout << "Comparing outputs..." << std::endl;
    compareOutputs(o, ref_output_data, B * H * N * D_VO, 1e-3f); // Adjust tolerance as needed
    std::cout << "Comparing kv_state..." << std::endl;
    compareOutputs(kv_state, ref_kv_state_data, B * H * STATE_PER_SM * COLLABORATIVE_SMS * D_QK * D_VO, 1e-3f); // Adjust tolerance as needed

    // Dump o to a text file
    std::ofstream outfile("output.txt");
    if (outfile.is_open()) {
        for (int i = 0; i < N * D_VO; i++) { // only dump one head
            outfile << __bfloat162float(o[i]) << " ";
            if ((i + 1) % 128 == 0) { // write out each line of o independently
                outfile << "\n";
            }
        }
        outfile.close();
        std::cout << "Output dumped to output.txt" << std::endl;
    } else {
        std::cerr << "Unable to open output file" << std::endl;
        return 1;
    }
    // Dump kv_state to a text file
    std::ofstream kv_state_file("kv_state.txt");
    if (kv_state_file.is_open()) {
        for (int i = 0; i < B * H * STATE_PER_SM * COLLABORATIVE_SMS * D_QK * D_VO; i++) { // only dump one head
            kv_state_file << kv_state[i] << " ";
            if ((i + 1) % 128 == 0) { // write out each line of o independently
                kv_state_file << "\n";
            }
        }
        kv_state_file.close();
        std::cout << "KV state dumped to kv_state.txt" << std::endl;
    } else {
        std::cerr << "Unable to open kv_state file" << std::endl;
        return 1;
    }

    // Free memory
    cudaFree(d_q);
    cudaFree(d_k);
    cudaFree(d_v);
    cudaFree(d_o);
    cudaFree(d_q_map);
    cudaFree(d_k_map);
    cudaFree(q_tma);
    cudaFree(k_tma);
    cudaFree(v_tma);
    cudaFree(o_tma);
    cudaFree(q_map_tma);
    cudaFree(k_map_tma);

    std::cout << "Execution completed successfully!" << std::endl;
    return 0;
}